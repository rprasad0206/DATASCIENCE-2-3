# -*- coding: utf-8 -*-
"""Group CaseStudy1_NewsAPI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G92Cujcw1DStYCxACp9Y6VhO_6B792PI

# Case Study 1 : Collecting Data from NewsAPI

Due Date: 11/10/2023, **BEFORE the beginning of class at 12:00pm EST**

## **NOTE: There are *always* last minute issues submitting the case studies.  DO NOT WAIT UNTIL THE LAST MINUTE!**

<img src="data:image/png;base64,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">

**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team

    Jacob Ruppert
    
    Ryan Dalton
    
    Rohan Prasad

    Bishoy Soliman Hanna

    Tobias Chin

*ALL QUESTIONS ANSWERED ON WRITE UP**

**Suggested Readings:**
* The NewsAPI documentation: https://newsapi.org/docs
* The Python library for NewsAPI: https://newsapi.org/docs/client-libraries/python
* The Pyhon notebook I used in class 3
  * In fact, it is intentional that many of these questions can be answered directly from there (except for question 4)!
* The idea is to ease you into the case studies :-)

**Don't forget!**
* You will need to install the newsapi-python library to access NewsAPI
 * pip install newsapi-python
* You will need to get an API key from NewsAPI to access the data
  * https://newsapi.org/register


** NOTE **
* **Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost**.

# Problem 1 (20 points): Sampling NewsAPI data a certain topic

* Select a topic that you are interested in, for example, "WPI" or "Lady Gaga"
* Use the NewsAPI to sample a collection of news articles about this topic. (It would be recommended that the number of articles should be larger than 200, but smaller than 100,000.
* Store the articles you downloaded into file on Google Drive (txt file or json file)
"""

! pip install newsapi-python

from newsapi import NewsApiClient
import requests


# See https://newsapi.org/docs for more information
# on NewsAPI implementation.

API_KEY = 'a8a965707bd943e6abf3c8b42d2249ee'

newsapi = NewsApiClient(api_key=API_KEY)
print(newsapi)

# Pulling all ESPN football articles from the past month
ball_articles = newsapi.get_everything(q='football',
                                      sources='ESPN',
                                      from_param='2023-11-01',
                                      to='2023-11-06',
                                      language='en',
                                      sort_by='popularity')
print(ball_articles)

"""### Report some statistics about the news articles you collected

* The topic of interest: ESPN Football Articles from between 11/1 and 11/9


* The total number of news articles collected:  219
"""

#----------------------------------------------
# Your code starts here
#   Please add comments or text cells in between to explain the general idea of each block of the code.
#   Please feel free to add more cells below this cell if necessary

# Imports drive and saves football articles to shared drive as football.txt
from google.colab import drive

drive.mount('/content/gdrive')

import json #please dont run it again the file has ben sent to everyone

data= json.dumps(ball_articles, indent=1)

file = open('gdrive/MyDrive/DS3010_Colab_Storage/football.txt','w')
file.write(data)
file.close()

# Imports the football.txt as d allowing us to analyze the articles without repeatedly calling the API.
from IPython.display import Image
from IPython.core.display import HTML

file = open('gdrive/MyDrive/DS3010_Colab_Storage/football.txt','r')
football = file.read()
d = json.loads(football)

# Shows a title and image of 3 of the articles in the dataset
for i in range(3):
  print(d['articles'][i]['title'])
  try:
    image = Image(url=d['articles'][i]['urlToImage'])
    display(image)
  except ValueError:
    print('no image')

# Heres the area for the 'statistics' we need for pt1
import time
from collections import Counter

authors = [ article['author'] for article in d['articles'] ]

sources = [ article['source']['name'] for article in d['articles'] ]

titles = [ article['title'] for article in d['articles'] ]

# Collection of words from all article titles, and the resulting top 10 most used words
words = [ w
          for d in titles
              for w in d.split() ]

for item in [authors, sources, words]:
    c = Counter(item)
    print(c.most_common()[:10]) # top 10

"""*-----------------------

# Problem 2 (20 points): Analyzing News Articles with Frequency Analysis

**1. Word Count:**
* Use the news articles you collected in Problem 1, and compute the frequencies of the words being used in these articles.
* Note, you can use various parts of the articles (e.g. the tite, the description, the content) to compute the word frequencies.
  * Which is most useful?
* Plot a table of the top 30 words with their counts

*Hint: One way to make very easy and asethestic way to make tables in Python is using the PrettyTable package. While using PrettyTable is not required for full credit, some starter code for using them is provided for Question 2.1 and 2.2*
"""

! pip install prettytable
from prettytable import PrettyTable

# Example Table
example_table = PrettyTable() # Initialize Table
example_table.field_names = ["Course Name", "Department"] # Declare column names

# Add each row
example_table.add_row(["DS3010", "Data Science"])
example_table.add_row(["CS4120", "Computer Science"])
example_table.add_row(["MIS 4084", "Business"])

# Display table
print(example_table)

#----------------------------------------------
#This creates a PrettyTable of the top 30 most used words in the titles of our articles
for label, data in (('Word', words),
                    # ('Author', authors),
                    # ('Source', sources)
                    ):
    football_table = PrettyTable(field_names=[label, 'Count'])
    c = Counter(data)
    [ football_table.add_row(kv) for kv in c.most_common()[:30] ]
    football_table.align[label], football_table.align['Count'] = 'l', 'r' # Set column alignment
    print(football_table)

"""**2. Find the most popular news sources and authors in your articles**

* Please provide a table of the top 10 authors and sources that are the most popular among your collection.

"""

#----------------------------------------------
#This creates a PrettyTable of the top 10 most popular authors/sources
for label, data in (('Author', authors),
                    ('Source', sources)
                    ):
    football_table = PrettyTable(field_names=[label, 'Count'])
    c = Counter(data)
    [ football_table.add_row(kv) for kv in c.most_common()[:10] ]
    football_table.align[label], football_table.align['Count'] = 'l', 'r'
    print(football_table)

"""**3. What is the lexical diversity in your descriptions and contents**

* For two popular authors or two popular sources, please compare the lexical diversity of their descriptions and contents.  
Which one is more diverse?  
  * Open ended questions: Does this make sense?  Why or why not? What might lexical diversity tell us about the author or source?
"""

# Define the lexical_diversity function
def lexical_diversity(tokens):
    return 1.0 * len(set(tokens)) / len(tokens)

# Load data from the text file
file_path = '/content/gdrive/MyDrive/DS3010_Colab_Storage/football.txt'
with open(file_path, 'r') as file:
    data = json.load(file)

# Initialize lists for descriptions, contents, authors, and sources
descriptions = []
contents = []
authors = []
sources = []

# Extract data from each article
for article in data['articles']:
    descriptions.append(article.get('description', ''))
    contents.append(article.get('content', ''))
    authors.append(article.get('author', ''))
    sources.append(article['source']['name'])

# Calculate lexical diversity for descriptions and contents
description_lexical_diversity = lexical_diversity(" ".join(descriptions).split())
content_lexical_diversity = lexical_diversity(" ".join(contents).split())

# Calculate the lexical diversity for authors and sources
author_lexical_diversity = {}
source_lexical_diversity = {}

for author in set(authors):
    author_articles = [content for content, a in zip(contents, authors) if a == author]
    author_lexical_diversity[author] = lexical_diversity(" ".join(author_articles).split())

for source in set(sources):
    source_articles = [content for content, s in zip(contents, sources) if s == source]
    source_lexical_diversity[source] = lexical_diversity(" ".join(source_articles).split())

# Find the two most popular authors
popular_authors = Counter(authors).most_common(2)

# Find the two most popular sources
popular_sources = Counter(sources).most_common(2)

# Compare lexical diversity for popular authors and sources
print("Lexical Diversity Comparison:")
for author, _ in popular_authors:
    print(f"Author: {author}, Lexical Diversity: {author_lexical_diversity.get(author, 0.0)}")

for source, _ in popular_sources:
    print(f"Source: {source}, Lexical Diversity: {source_lexical_diversity.get(source, 0.0)}")

#------------------------------------------------------------------------------------#
# A lexical diversity score of 0 indicates that there is no diversity,
# meaning that every word in the text is the same.
#  This would happen if the text contains only one unique word repeated multiple times.

# A lexical diversity score of 1 indicates the highest level of diversity,
# where every word in the text is unique, and there is no repetition of words.

"""# Problem 3 (20 points): Image processing

* For each news article, download the image associated with it (using the urlToImage field).
* Do these images tell you anything about the news articles?  If so, what?
"""

#----------------------------------------------
# Installing and setting up huggingface
! pip install -q datasets transformers evaluate timm albumentations

from huggingface_hub import notebook_login

notebook_login()

import json
import requests

from IPython.display import Image, display

articles = d['articles']

# Download and display the images
for idx, article in enumerate(articles):
    image_url = article.get('urlToImage')
    if image_url:
        response = requests.get(image_url)
        if response.status_code == 200:
            image_data = response.content
            image_display = Image(data=image_data)
            display(image_display)

"""* Let's automate the process!  Use Hugging Face's transformers library to download a pre-trained image segmentation model.
  * See my notebook from class 3 for an example of how to do this.
* Open ended question: Collect statistics on what appears in the images for your news articles? What does this tell you?
"""

pip install -q datasets transformers evaluate timm albumentations

from huggingface_hub import notebook_login

notebook_login()

#----------------------------------------------
# Actual image processing is carried out in problem 4: Business question

from transformers import pipeline #DonkeyWT/Football
import requests
from PIL import Image, ImageDraw

image_urls = [article["urlToImage"] for article in articles[:10]]
image_data_list = [Image.open(requests.get(url, stream=True).raw) for url in image_urls if url]

obj_detector = pipeline("object-detection")

"""

# Problem 4 (20 points): Business question

Run some additional experiments with your data to gain familiarity with the NewsAPI.

* Come up with a business question that NewsAPI data could help answer.
* Decribe the business case.
* How could NewsAPI data help a company decide how to spend its resources."""

#Our question is: How can common trends in ESPN articles be used to help advertisers better appeal to ESPN's demographic?
#We further examine this question and our findings from this Colab in our case study report and presentation.

from PIL import Image, ImageDraw
from transformers import pipeline
import requests

# Initialize object detection pipeline with DETR model
object_detector = pipeline("object-detection", model="facebook/detr-resnet-101-dc5")

# Assuming 'image_urls' have the URLs of your images
for image_url in image_urls:
    # Load the image
    if image_url.startswith("http"):
        image = Image.open(requests.get(image_url, stream=True).raw)
    else:
        image = Image.open(image_url)

    # Use Hugging Face object detection pipeline
    results = object_detector(image)

    # Draw bounding boxes on the image
    draw = ImageDraw.Draw(image)

    # Get the predictions and draw bounding boxes
    for result in results:
      score, label, box = result["score"], result["label"], result["box"]
      x, y, x2, y2 = box['xmin'], box['ymin'], box['xmax'], box['ymax'],
      draw.rectangle((x, y, x2, y2), outline="red", width=1)
      draw.text((x, y), label, fill="white")

    # Save or display the processed image
    image_path = f'path_to_save_result_{image_url.split("/")[-1]}.png'
    image.save(image_path)
    display(image)

"""# Done

All set!

** What do you need to submit?**

* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, "filename.ipynb"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used "ipython notebook --pylab=inline" to open the notebook, all the figures and tables should have shown up in the notebook.


* **PPT Slides**: please prepare PPT slides (for 15 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study.

* ** Report**: please prepare a report (less than 10 pages) to report what you found in the data.
    * What data you collected?
    * Why this topic is interesting or important to you? (Motivations)
    * How did you analyse the data?
    * What did you find in the data?

     (please include figures or tables in the report, but no source code)

Please compress all the files in a zipped file.


** How to submit: **

        Please submit through email to Prof. Paffenroth (rcpaffenroth@wpi.edu) *and* the TA (???).

#### We auto-process the submissions so make sure your subject line is *exactly*:

### DS3010 Case Study 1 Team 1
        
** Note: Each team just needs to submits one submission **

# Grading Criteria:

** Totoal Points: 100 **


---------------------------------------------------------------------------
** Notebook results:  **
    Points: 80


    -----------------------------------
    Qestion 1:
    Points: 20
    -----------------------------------
    
    (1) Select a topic that you are interested in.
    Points: 6
    
    (2) Use the NewsAPI to sample a collection of news articles about this topic. (It would be recommended that the number of articles should be larger than 200, but smaller than 100,000. Please check whether the total number of tweets collected is larger than 200.
    Points: 10
    
    
    (3) Store the articles you downloaded into a file on Google Drive (txt file or json file)
    Points: 4
    
    
    -----------------------------------
    Qestion 2:
    Points: 20
    -----------------------------------
    
    1. Word Count

    (1) Use the news articles you collected in Problem 1, and compute the frequencies of the words being used in these articles.
    Points: 5

    (2) Plot a table of the top 30 words with their counts
    Points: 5
    
    2. Please provide a table of the top 10 authors and sources that are the most popular among your collection.
    Points: 5
    
    3. For two popular authors or two popular sources, please compare the lexical diversity of their descriptions and contents.
    Points: 5
      
    -----------------------------------
    Qestion 3:
    Points: 20
    -----------------------------------
    
    (1) For each news article, download the image associated with it (using the urlToImage field).
    Points: 10

    (2) Use Hugging Face's transformers library to download a pre-trained image segmentation model.
    Points: 10
  
    -----------------------------------
    Qestion 4:  Business question
    Points: 20
    -----------------------------------
        Novelty: 10
        Interestingness: 10
    -----------------------------------
    Run some additional experiments with your data to gain familiarity with the NewsAPI.  Come up with a business question and describe how NewsAPI data can help you answer that question.



---------------------------------------------------------------------------
**Slides (for presentation): Story-telling**
    Points: 20


1. Motivation about the data collection, why the topic is interesting to you.
    Points: 5

2. Communicating Results (figure/table)
    Points: 10

3. Story telling (How all the parts (data, analysis, result) fit together as a story?)
    Points: 5
"""